{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password2\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = psycopg2.connect(\n",
    "    database = \"postgres\",\n",
    "    user = \"postgres\",\n",
    "    password = db_password2,\n",
    "    host = \"js-ucbfinalprojectdb.c8s02fywjegc.us-east-2.rds.amazonaws.com\",\n",
    "#     host = \"dataviz-db.csxrf9ti2aba.us-east-2.rds.amazonaws.com\",\n",
    "    port = 5432)\n",
    "cursor = engine.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Pandas to see all data in table\n",
    "sql = \"\"\"\n",
    "SELECT \"table_name\",\"column_name\", \"data_type\", \"table_schema\"\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE \"table_schema\" = 'public'\n",
    "ORDER BY table_name  \n",
    "\"\"\"\n",
    "pd.read_sql(sql, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull all info into pandas dataframe from RDS dB\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "from bigquery_totals\n",
    "\"\"\"\n",
    "bigquery_totals_df = pd.read_sql(sql,con = engine)\n",
    "#plug NaN's as 0's\n",
    "bigquery_totals_df = bigquery_totals_df.fillna(0)\n",
    "print(bigquery_totals_df.shape)\n",
    "bigquery_totals_df.head(n = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Drop in Machine Learning libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_totals_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_totals_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets work on reducing value dimensions in some of our columns to do oneHotEncoding\n",
    "# print(bigquery_totals_df.browser.value_counts())\n",
    "# # We can reduce by counting all browser values under 100 as OTHER\n",
    "\n",
    "# print(bigquery_totals_df.subcontinent.value_counts())\n",
    "# # We can reduce by count all subcontinent values under 500 as OTHER\n",
    "\n",
    "# print(bigquery_totals_df.country.value_counts())\n",
    "# bigquery_totals_df.country.value_counts().plot.density()\n",
    "# bigquery_totals_df.country.value_counts()[bigquery_totals_df.country.value_counts()>250]\n",
    "# # We can reduce by count of all countries values under 250 as OTHER\n",
    "\n",
    "# print(bigquery_totals_df.region.value_counts())\n",
    "# bigquery_totals_df.region.value_counts()[bigquery_totals_df.region.value_counts()>100]\n",
    "# # A lot of \"not available in demo dataset\". I think we should remove this column.\n",
    "\n",
    "# bigquery_totals_df.metro.value_counts()\n",
    "# # A lot of \"not available in demo dataset\". I think we should remove this column.\n",
    "\n",
    "# bigquery_totals_df.city.value_counts()\n",
    "# # A lot of \"not available in demo dataset\". I think we should remove this column.\n",
    "\n",
    "# bigquery_totals_df.referralpath.value_counts()\n",
    "# bigquery_totals_df.referralpath.value_counts()[bigquery_totals_df.referralpath.value_counts()>10]\n",
    "# # Unsure how well this column will be in our model. I think we should remove this column \n",
    "\n",
    "# bigquery_totals_df.source.value_counts()\n",
    "# bigquery_totals_df.source.value_counts()[bigquery_totals_df.source.value_counts()>50]\n",
    "# # We can reduce by count of all sources under 50 as OTHER\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACEEEEEEEEEEEEEEE\n",
    "replace_browser_types = list(bigquery_totals_df.browser.value_counts()[bigquery_totals_df.browser.value_counts()<100].index)\n",
    "replace_subcontinent_types = list(bigquery_totals_df.subcontinent.value_counts()[bigquery_totals_df.subcontinent.value_counts()<500].index)\n",
    "replace_country_types = list(bigquery_totals_df.country.value_counts()[bigquery_totals_df.country.value_counts()<250].index)\n",
    "replace_source_types = list(bigquery_totals_df.source.value_counts()[bigquery_totals_df.source.value_counts()<50].index)\n",
    "\n",
    "\n",
    "replace_transactions_values = list(bigquery_totals_df.transactions.value_counts().index)\n",
    "replace_transactions_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace in dataframe\n",
    "for browser_replace in replace_browser_types:\n",
    "    bigquery_totals_df.browser = bigquery_totals_df.browser.replace(browser_replace,\"Other\")\n",
    "\n",
    "for subcontinent_replace in replace_subcontinent_types:\n",
    "    bigquery_totals_df.subcontinent = bigquery_totals_df.subcontinent.replace(subcontinent_replace,\"Other\")\n",
    "    \n",
    "for country_replace in replace_country_types:\n",
    "    bigquery_totals_df.country = bigquery_totals_df.country.replace(country_replace,\"Other\")\n",
    "    \n",
    "for source_replace in replace_source_types:\n",
    "    bigquery_totals_df.source = bigquery_totals_df.source.replace(source_replace,\"Other\")\n",
    "    \n",
    "# Our predicting variable transactions, is an integer value for total number of ecommerce transactions. We want to make this a categorical variable\n",
    "for trans_vals in replace_transactions_values:\n",
    "    if trans_vals > 0.0:\n",
    "        bigquery_totals_df.transactions[bigquery_totals_df.transactions == trans_vals] = 1\n",
    "    elif trans_vals == 0.0:\n",
    "        bigquery_totals_df.transactions[bigquery_totals_df.transactions == trans_vals] = 0\n",
    "    \n",
    "bigquery_totals_df.transactions = bigquery_totals_df.transactions.astype(int)\n",
    "#bigquery_totals_df.transactions.unique() \n",
    "bigquery_totals_df.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of the categorical variables\n",
    "bigquery_totals_cat = bigquery_totals_df.dtypes[bigquery_totals_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "\n",
    "# From the list of categorical variables we DONT want some of them. We'll .remove() the columns we want from the list\n",
    "removal_list = ['fullvisitorid','region','metro','city','referralpath','date']\n",
    "\n",
    "for elem in removal_list:\n",
    "    bigquery_totals_cat.remove(elem)\n",
    "\n",
    "\n",
    "# Fix Boolean istruedirect column from True/None to 1/0\n",
    "# bigquery_totals_df[\"istruedirect\"] = bigquery_totals_df[\"istruedirect\"].astype(int)\n",
    "\n",
    "# #get number of unique values per category column\n",
    "bigquery_totals_df[bigquery_totals_cat].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_totals_df.dtypes[bigquery_totals_df.dtypes == \"object\"].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slap a OneHotEncoding on this for categorical columns\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "#fit and transform the encoding using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(bigquery_totals_df[bigquery_totals_cat]))\n",
    "\n",
    "#add the encoded variable names to the DataFrame\n",
    "encode_df.columns = enc.get_feature_names(bigquery_totals_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge one-hot encoded features and drop the originals\n",
    "bigquery_totals_df1 = bigquery_totals_df.merge(encode_df,left_index = True, right_index = True)\n",
    "bigquery_totals_df1 = bigquery_totals_df1.drop(columns = bigquery_totals_cat+removal_list)\n",
    "#bigquery_hits_df1 = bigquery_hits_df1.drop(columns = remove_these_columns)\n",
    "bigquery_totals_df1.head()\n",
    "#bigquery_hits_df1.productprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Action type from our features dataset\n",
    "y = bigquery_totals_df1.transactions\n",
    "\n",
    "X = bigquery_totals_df1.drop(columns =['transactions','totaltransactionrevenue','transactionrevenue'])\n",
    "\n",
    "# Split training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1337,stratify = y)\n",
    "\n",
    "#y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling to boost our test variable!\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# What we'll do is rejoin our traning data set back together, and then resample data from the training data\n",
    "# such that we have an equal number of transactions (value of 1) and no transactions (value of 0)\n",
    "\n",
    "# Concatenate our training data back together\n",
    "X = pd.concat([X_train,y_train], axis = 1)\n",
    "\n",
    "# Separate minority and majority classes\n",
    "not_transaction = X[X.transactions == 0]\n",
    "transaction = X[X.transactions == 1]\n",
    "\n",
    "# Upsample minority\n",
    "transaction_upsampled = resample(transaction,\n",
    "                                replace = True, # sample with replacement\n",
    "                                n_samples = len(not_transaction), # match number in majority class\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "#combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_transaction,transaction_upsampled])\n",
    "\n",
    "# check new transaction counts\n",
    "upsampled.transactions.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling to even our test variable!\n",
    "# use the same tranasction and not_transaction from above\n",
    "\n",
    "# downsampled majority\n",
    "not_transaction_downsampled = resample(not_transaction,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(transaction), # match number in minority class\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# Combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_transaction_downsampled,transaction])\n",
    "\n",
    "# Checking counts\n",
    "downsampled.transactions.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to our upsampled/downsampled data\n",
    "# Upsampled\n",
    "y_train = upsampled.transactions\n",
    "X_train = upsampled.drop('transactions',axis = 1)\n",
    "\n",
    "# Downsampled\n",
    "# y_train = downsampled.transactions\n",
    "# X_train = downsampled.drop('transactions',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standar scaler time lets get it\n",
    "\n",
    "#create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for all Machine Learning methods. This will help compare results\n",
    "def run_ml_model(x_data_train,x_data_test,y_data_train,y_data_test,model,n_runs):\n",
    "    my_cm = 0\n",
    "    my_acc = 0\n",
    "    my_f1 = 0\n",
    "    my_recall = 0\n",
    "    if model == 'Logistic Regression':\n",
    "        # Define the logistic regression model\n",
    "        log_classifier = LogisticRegression(solver=\"lbfgs\",max_iter=n_runs)\n",
    "\n",
    "        # Train the model\n",
    "        log_classifier.fit(x_data_train,y_data_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = log_classifier.predict(x_data_test)\n",
    "\n",
    "        my_cm = metrics.confusion_matrix(y_data_test,y_pred)\n",
    "        my_acc = accuracy_score(y_data_test,y_pred)\n",
    "        my_f1  = f1_score(y_data_test,y_pred)\n",
    "        my_recall = recall_score(y_data_test,y_pred)\n",
    "        \n",
    "    if model == 'Random Forest':\n",
    "        # Create a random forest classifier.\n",
    "        rf_model = RandomForestClassifier(n_estimators=n_runs, random_state=1337)\n",
    "\n",
    "        # Fitting the model\n",
    "        rf_model = rf_model.fit(x_data_train, y_data_train)\n",
    "\n",
    "        # Evaluate the model\n",
    "        y_pred = rf_model.predict(x_data_test)\n",
    "        my_cm = metrics.confusion_matrix(y_data_test,y_pred)\n",
    "        my_acc = accuracy_score(y_data_test,y_pred)\n",
    "        my_f1  = f1_score(y_data_test,y_pred)\n",
    "        my_recall = recall_score(y_data_test,y_pred)\n",
    "    \n",
    "    if model == 'Neural Network':\n",
    "        # Define the basic neural network model\n",
    "        nn_model = tf.keras.models.Sequential()\n",
    "        nn_model.add(tf.keras.layers.Dense(units=25, activation=\"tanh\", input_dim=124))\n",
    "        nn_model.add(tf.keras.layers.Dense(units=10, activation=\"relu\"))\n",
    "\n",
    "        # Compile the Sequential model together and customize metrics\n",
    "        nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train the model\n",
    "        fit_model = nn_model.fit(x_data_train, y_data_train, epochs=n_runs)\n",
    "\n",
    "        y_pred = nn_model.predict_classes(x_data_test,verbose = 1)\n",
    "\n",
    "        # Evaluate the model using the test data\n",
    "        nn_model.evaluate(x_data_test,y_data_test,verbose=0)\n",
    "        my_cm = metrics.confusion_matrix(y_data_test,y_pred)\n",
    "        my_acc = accuracy_score(y_data_test,y_pred)\n",
    "        my_f1  = f1_score(y_data_test,y_pred)\n",
    "        my_recall = recall_score(y_data_test,y_pred)\n",
    "        \n",
    "    #return print(cm), print(f\" Logistic regression model accuracy: {accuracy_score(y_data_test,y_pred):.3f}\"), print(f\" F1 Score: {f1_score(y_data_test,y_pred):.3f}\"),print(f\" Recall Score: {recall_score(y_data_test,y_pred):.3f}\")\n",
    "    return my_cm, my_acc, my_f1,my_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====== Logistic Regression')\n",
    "log_cm,log_acc,log_f1,log_recall = run_ml_model(X_train_scaled,X_test_scaled,y_train,y_test,'Logistic Regression',n_runs = 10000)\n",
    "print(log_cm,'\\n',f\" Logistic regression model accuracy: {log_acc:.3f}\",'\\n',f\" F1 Score: {log_f1:.3f}\",'\\n',f\" Recall Score: {log_recall:.3f}\")\n",
    "print('===========================\\n')\n",
    "\n",
    "print('====== Random Forest')\n",
    "rf_cm,rf_acc,rf_f1,rf_recall = run_ml_model(X_train_scaled,X_test_scaled,y_train,y_test,'Random Forest',n_runs = 50)\n",
    "print(rf_cm,'\\n',f\" Random Forest model accuracy: {rf_acc:.3f}\",'\\n',f\" F1 Score: {rf_f1:.3f}\",'\\n',f\" Recall Score: {rf_recall:.3f}\")\n",
    "print('===========================\\n')\n",
    "\n",
    "print('====== Neural Network')\n",
    "nn_cm,nn_acc,nn_f1,nn_recall = run_ml_model(X_train_scaled,X_test_scaled,y_train,y_test,'Random Forest',n_runs = 200)\n",
    "print(nn_cm,'\\n',f\" Neural Network model accuracy: {nn_acc:.3f}\",'\\n',f\" F1 Score: {nn_f1:.3f}\",'\\n',f\" Recall Score: {nn_recall:.3f}\")\n",
    "print('===========================\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
